#Diving into the internals of the conntrack

### Путь пакетов через ядро и подсистема netfilter

Каждый кадр/ячейка/сетевой пакет (далее мы просто пакет, но надо учитывать контекст, чтобы не было путаницы) представлен в ядре двумя сущностями: собственно сами данные кадра/ячейки/пакета и структурой socket buffer, которая хранит в себе различную метаинформацию как о самом пакете (протокол, длина, указатели за различные заголовки, интерфейсы, через которые пакет пришёл или уйдёт и т.п.), так и внутриядерные служебные данные, ассоциированные с этим пакетом (счётчик ссылок, указатели на элементы списка для GSO, привязка к ЦПУ и очередям и т.п.). В ядре эта структура объявлена следующим образом - [тыц](http://lxr.free-electrons.com/source/include/linux/skbuff.h?v=4.7#L626) - там довольно-таки всё интуитивно понятно названо и, порой, даже прокоментировано. Более подробно человеческим языком о skbuff написано [тут](http://vger.kernel.org/~davem/skb.html). Это нам пригодится для дальнейшего погружения в потроха ядра.

Когда сетевая карта получает кадр по сети, она обычно размещает его в памяти (как правило используя DMA и кольцевой буфер), драйвер заполняет необходимые поля соответствующей пакету структуры skbuff и передаёт её дальше в сетевой стек, используя прерывание. Вот тут-то и начинается самое интересное, хе-хе.

Эта структура проходит через множество этапов обработки, передаётся в функции обработчиков протоколов различного уровня, у неё перезаписываются различные поля, перезаписываются сами данные пакета, и т.д., и т.п. Если нам нужен крутой файерволл, который может много всего, то он обязательно должен уметь обрабатывать трафик на различных этапах  обработки. А чтобы это делать, нужно этот трафик как-то перехватывать на всех этих этапах, и уже передавать в наш файерволл. В ядре линукс этот перехват трафика и реализован в подсистеме с названием netfilter. Эта подсистема предоставляет два внутриядерных интефейса (набора функций, структур и определений функций): один интерфейс для драйверов и обработчиков протоколов, благодаря чему разработчик может предусмотреть перенаправление трафика на обработку файерволлу, а второй интерфейс - для получения трафика на различных этапах обработки - для самой реализации файерволла. Сейчас в ядре существует две реализации файерволла - xtables, которая пришла на смену ipchains, и nftables, которая сейчас очень активно разрабатывается, и вообще суперпрогрессивная и производительная, в которой учтено большинство шишек, набитых во время реалиазации предыдущих реализаций (ух, как сложно и маркетинг-буллшитно получилось!).

Каждый этап обработки сетевого трафика представлен в netfilter в виде хука - точки перехвата. На следующей картинке я кратко нарисовал схему прохождения трафика через эти хуки для ipv4/ipv6 трафика на сетевом уровне (для ethernet трафика на уровне коммутации кадров названия хуков будут немного отличаться, но принцип организации тот же; более полная схема для для всех уровней и с указанием цепочек лежит на [википедии](https://upload.wikimedia.org/wikipedia/commons/3/37/Netfilter-packet-flow.svg)).

![Путь пакета через сетевой стек](https://github.com/veryangryman/articles/raw/master/images/p_flow_brief2.png)

Кратко опишу вышеуказанные хуки, и их предназначение.

1. __INGRESS__ - перехват на самом раннем этапе обработки трафика. Этот хук появился в ядре, начиная с версии 4.2 (т.е. относительно совсем недавно), и является альтернативой tc ingress queue, используемой для различных манипуляций с входящим трафиком (классификация, полисинг, перезапись, зеркалирование и перенаправление, и т.п.) Особенностью данного хука является то, что он привязывается к каждому сетевому интерфейсу независимо и опционально. Поддерживается только из nftables и позволяет делать ещё более интересные штуки с входящим трафиком (подробности и примеры использования в [списке рассылки](http://marc.info/?l=netfilter-devel&m=143033337020328&w=2) - выглядит очень вкусно).

1. __PREROUTING__ - на этом этапе системе ещё неизвестно назначение пакета, благодаря чему мы можем повлиять на принятие решения о маршрутизации с помощью перезаписи адресов и меток. Так как дальнейший путь трафика неизвествен, то, соответственно, мы не знаем выходной интерфейс, и по этой причине мы не можем использовать сопоставление по выходному интерфейсу (__--output-interface__ в правилах iptables). После этого хука выполняется поиск маршрута (route lookup). Для ipv4/ipv6 ищется соответствующий маршрут с применением правил маршрутизации (policy based routing), а так же пакет опционально пропускается через фильтр обратного пути (reverse path filter - о нём можно прочитать [здесь](http://lxr.free-electrons.com/source/Documentation/networking/ip-sysctl.txt#L1089)), который с недавнего времени перекочевал в файерволл. В зависимости от типа маршрута, трафик может быть как предназначен как самому хосту (типы маршрутов __local__/__broadcast__/__multicast__), так и транзитным (маршруты типов __unicast__/__multicast__), а может и вообще быть отброшен (маршруты типов __blackhole__/__unreachable__/__prohibit__). Далее трафик пройдёт через различные хуки, в соответствии дальнешему маршруту.

1. __INPUT__ - через этот хук проходит трафик, предназначенный самом хосту. Узнав на предыдущем этапе, что пакеты адресованы самому хосту, трафик передаётся обработчику инкапсулированного протокола (это может быть tcp/udp/sctp/ipsec и т.п.). Например, для транспортных протоколов ищется соответствующий сокет, данные декапсулируются и передаются в найденый сокет. А для IPSEC трафик пропускается через SPDB (база данных политик безопасности), происходит его декапсуляция и расшифровка, а затем он уже в открытом виде инжектируется на этап перед хуком __PREROUTING__. Таким образом, некоторые типы трафика проходят через некоторые хуки дважды в различном виде.

1. __FORWARD__ - через этот хук проходит транзитный трафик, т.е. трафик, который мы пересылаем дальше в сеть другому хосту. Обычно на этом этапе ничего интересного с трафиком не происходит, и он не покидает сетевого уровня. Зная дальнейший путь движения, мы уже можем в файерволлах использовать сопоставление по выходному интерфейсу (__--output-interface__).

1. __OUTPUT__ - только трафик, оригинированный данным хостом, проходит через этот хук. Для новых соединений мы сразу знаем выходной интерфейс и адрес источника, так как решение о маршрутизации принимается в момент передачи данных через сокет от процесса в сетевой стек, при инкапсулации данных на сетевом уровне. Так же тут выполняется перепроверка решения о маршрутизации (reroute check), так мы можем на него повлиять с помощью DNAT, меток и политик маршрутизации. В общем, тоже ничего необычного и суперинтересного.

1. __POSTROUTING__ - последний хук, через который пропускается сетевой трафик. Так же после этого хука трафик может быть пропущен через SPDB, зашифрован и инкапсулирован в IPSEC, а затем инжектирован на этап в районе хука __OUTPUT__. После __POSTROUTING__ трафик передаётся в подсистему __TC__, а далее уже в драйвер сетевого интерфейса.

Мы рассмотрели кратко эти хуки. Но хочется знать как они внутри устроены. На самом деле всё довольно просто оказалось. Каждый хук представляет собой список. Элементы списка - структуры типа [nf_hook_ops](http://lxr.free-electrons.com/source/include/linux/netfilter.h?v=4.7#L87), которые содержат в себе указатель на функцию обратного вызова и приоритет, и ещё немного всякого разного. Эти структуры внутри списка упорядочены по приоритету.

Когда трафик проходит через различные этапы обработки, и его надо передать в файерволл, то в конечном итоге [происходит](http://lxr.free-electrons.com/source/net/netfilter/core.c?v=4.7#L295) [следующее](http://lxr.free-electrons.com/source/net/netfilter/core.c?v=4.7#L256): мы обходим соответствующий хуку список в порядке приоритета элементов, вызываем по указателю коллбеки, передавая в них сам пакет и некоторые дополнительные данные.

Каждый коллбек возвращает целочисленный [результат](http://lxr.free-electrons.com/source/include/uapi/linux/netfilter.h?v=4.7#L10), в зависимости от того, что в нём произошло, а так же модифицирует состояние хука, ассоциированное с данным пакетом:

1. __NF_DROP__ - отбросить пакет.
1. __NF_ACCEPT__ - пакет прошёл через данный коллбек, и будет пропущен через следующие коллбеки.
1. __NF_STOLEN__ - пакет удерживается внутри коллбека и будет возвращён позже (а может и нет), т.е. есть коллбек изъял пакет из дальнейшей обработки в сетевом стеке. Это используется, например, в коде сборки фрагментов в __conntrack__ и в реализации __SYN-прокси__.
1. __NF_QUEUE__ - пакет направляется в __netfilter queue__, и будет обработан в юзерспейсе, а потом возвращён обратно.
1. __NF_REPEAT__ - пропустить пакет через этот же коллбек ещё раз.
1. __NF_STOP__ - назначение этого результата почти такое же, как и __NF_ACCEPT__, но пакет уже не пройдёт через следующие коллбеки.

Если же мы пишем свой файерволл и хотим перехватывать трафик через хуки, то мы регистрируем свой коллбек в заданном хуке с заданным приоритетом. Переданная структура вставляется в соответствующий хуку список в нужную позицию.

Если гипотетически увеличить один из хуков на схеме (возьмём __PREROUTING__ для наглядности), то получим что-то вроде следующего (для протокола ipv4 для системы с загруженными модулями conntrack, conntrack_defrag, raw и mangle): ![детализация хука PREROUTING](https://github.com/veryangryman/articles/raw/master/images/prert_dets.png)

Все эти страшные __NF_IP_PRI_RAW__ и т.п. являются целочисленными знаковыми константами (их можно посмотреть так же в [исходниках](http://lxr.free-electrons.com/source/include/uapi/linux/netfilter_ipv4.h?v=4.7#L57)). Это список будет меняться в зависимости от загруженных модулей, которые при загрузке регистрируют свои функции в хуке, а при выгрузке - разрегистрируют.

Вот этим всем собственно и занимается netfilter. А уже поверх этой инфраструктуры хуков и коллбеков и реализуется подсистемы conntrack, xtables и nftables.

Из всего вышеследующего следует несколько неочевидных, но эффективных трюков, позволяющих улучшить производительность вашего файерволла:

1. Не загружать лишние модули, которые регистрируют свои коллбеки в хука, которые не используете. Меньше функций в цепочке хуков - меньше вызовов коллбеков, которые всё равно ничего не делают. Но даже если функция ничего не делает, то она всё равно вызывается, а для этого хоть и требуется совсем чуть-чуть ресурсов, но такты процессора расходуются.

1. Если используете динамические чёрные списки (например, по рейтлимиту или создаваемые с помощью fail2ban), то поместите правило с таргетом DROP в цепочку raw/PREROUTING - так пакеты будут отбрасываться на самом раннем этапе обработке - вы экономите ресурсы на отказе от поиска маршрутов, проверках с помощью reverse path filter, а так же прохождения этих пакетов через начальные этапы обработки в __conntrack__. В случае же __nftables__ вы можете зарегистрировать ещё более раннюю цепочку с приоритетом ещё меньшим, чем __NF_IP_PRI_CONNTRACK_DEFRAG__, и дропать трафик по блек-листу ещё раньше, сэкономив ресурсы ещё на проверку и сборку фрагментов в __conntrack__.

### Краткий обзор, что такое conntrack

__Conntrack__ - сокращение от connection tracker - по-русски "отслеживатель соединений". Это реализация так называемого stateful firewall - файерволла с учётом состояний (статья на [вики](https://ru.wikipedia.org/wiki/Stateful_Packet_Inspection)). С помощью __conntrack__ реализован так же __stateful NAT__(внезапно).

Говоря простым языком (хе-хе, насколько это возможно), стейтфул файерволл проверяя отдельные пакеты, ассоциирует их с потоками трафика, отслеживая состояние этих потоков. С помощью такого механизма очень легко, например, разрешить инициировать соединения только в определённом направлении (от узла А во внешние сети, но запретить входящие соединения к узлу А из внешних сетей), что очень полезно для DMZ и прочего.

Так же без __conntrack__ невозможен полноценный __NAT__ из-за того, что некоторые протоколы открывают дополнительные соединения, передавая информацию о нижележащих протоколов в данных протокола более высокого уровня, так что для них простой подменой адресов/номеров портов не обойтись. Канонический пример такого дизайна - протокол FTP, который:
* Использует одно управляющее соединение и открывает дополнительные соединения для передачи данных.
* Открывает эти дополнительные соединения в двух режимах (отличающихся тем, какая из сторон инициирует новое соединения, а какая открывает порты для того, чтобы это соединение принять), передавая адреса и номера портов внутри контрольного соединения. Так что в случае NAT нам мало того, что нужно разрешить эти дополнительные коннекты, так ещё и перезаписывать адресную информацию внутри контрольного соединения!

К другим подобным сложным протоколам относится SIP (открывает коннекты для передачи голосовых данных), H.323, tftp, pptp, irc и т.п.

Естественно, такое реализовать не совсем просто, так как нужно разбирать одиночные пакеты, парсить заголовки каждого уровня вплоть до прикладного, сопоставлять эти пакеты с потоком, информацию о котором надо хранить на всё протяжении его жизни, да ещё уметь выполнять дополнительные манипуляции с данными самого пакета на самых разных уровнях. Это вам не это (с).

К дополнению к возможностям по самой фильтрации, нам ещё нужно уметь реплицировать базу данных отслеживаемых соединений между несколькими узлами для того, чтобы реализовать кластер файерволлов для масштабирования. __conntrack__ это умеет тоже.

### Таблицы conntrack

Отслеживая пакеты и ассоциируя их с потоками (соединениями), нам нужно эти все соединения как-то и где-то хранить, да ещё сделать так, чтобы искать нужные соединения очень быстро.

В качестве таблицы соединений используется хеш-таблица. Для разрешения коллизий в хеш-таблице используются списки (т.н. [separate chaining](https://en.wikipedia.org/wiki/Hash_table#Separate_chaining)). Для синхронизации доступа к ячейкам таблицы (англ. buckets) используется массив [спинлоков](https://ru.wikipedia.org/wiki/Spinlock), причём их фиксированное количество - [1024](http://lxr.free-electrons.com/source/net/netfilter/nf_conntrack_core.c?v=4.7#L66) на всю таблицу, таким образом один спинлок синхронизирует доступ к нескольким ячейкам. И ещё есть [дополнительный спинлок](http://lxr.free-electrons.com/source/net/netfilter/nf_conntrack_core.c?v=4.7#L76) для глобальной блокировки всей таблицы, чтобы нам избежать цикла для захвата/освобождения всех локов в массиве.

Дефолтный размер хеш-таблицы (количество ячеек) вычисляется довольно-таки [интересно](http://lxr.free-electrons.com/source/net/netfilter/nf_conntrack_core.c?v=4.7#L1713): для  гигабайт размер будет равен 65536, для памяти от 1 до 4 гигабайт 16384 ячейки, для систем с объёмом памяти до 1 гигабайта количество ячеек будет вычисляться по формуле <RAMSIZE> / 16384 / <размер заголовка списка>, но не менее 32 ячеек. Вот такая вот магия.

Размер хеш-таблицы можно менять в процессе работы, но надо учесть, что это очень затратная операция в плане производительности, так как мы сначала создаём новую хеш-таблицу с заданного размера, затем копируем все соединения из старой таблицы, пересчитывая их хеши, а потом уже удаляем старую пустую таблицу. Так как непосредственный перелив данных о потоках в новую хеш-таблицу происходит под блокировкой [RCU](https://lwn.net/Articles/262464/), то мы сохраняем возможность работать с таблицей потоков во время её ресайза. Так-то.

Максимальное количество элементов в хеш-таблице вычисляется по формуле **HTSize>** * **max_factor**, где **max_factor** - максимальное количество элементов в цепочке в каждой ячейке. Это так называемый фактор хеш-таблицы. Если у нас в каждой цепочки хеш-таблицы будет много элементов, то, ественно, чтобы найти нужную запись в них, понадобится больше времени, так как мы не можем перейти сразу к нужному соединению в цепочке, так как местоположение этого соединения неизвестно (только значение хеша - ячейка таблицы), и вынуждены двигаться по цепочке с самого начала (обходить список; так же это называется итерированием), проверяя каждый элемент.

В виду вышеизложенного, идеальным будет являться случай, когда у нас в каждом списке каждой ячейки будет по одному элементу, так как в таком случае нам не нужно долго-долго обходить списки, проверяя их в поисках нужного элемента, которого, кстати, может там и не оказаться. Гарантированно нам этого не добиться, так как всё равно могут возникать коллизии (разные потоки могут иметь иметь одинаковый хеш), но мы можем приблизиться к этому.

Так же, если вы обрабатываете очень большой объём трафика, то необходимо будет увеличить лимит на суммарное количество соединений в таблице коннтрака, иначе в один прекрасный момент можно получить дропы с ошибкой "nf conntrack: table full. dropping packet".

Сам лимит на количество потоков в таблице лежит в переменной __nf_conntrack_max__. Но увеличивая её вы не увеличиваете количество ячеек в таблице, а увеличение происходит лишь за счёт увеличения фактора хеш-таблицы (количество элементов в цепочке, принадлежащей одной ячейке). Очевидно, что это не даст прирост производительности, так как чтобы обойти более длинную цепочку, нам понадобится больше времени, но зато вы избавитесь от ошибки переполнения таблицы. Само количество ячеек хранится в переменной __hashsize__ - опции модуля __ip_conntrack__. Его можно выставить либо через опции при загрузке ядра (__nf_conntrack.hashsize__), либо через запись в __/sys/module/nf_conntrack/parameters/hashsize__. А затем уже увеличивайте __nf_conntrack_max__.

Из вышеописанного следует: первым делом увеличивайте __hashsize__(оно же __nf_conntrack_buckets__), а потом уже __nf_conntrack_max__, если нужно. Это позволит снизить потребление ЦПУ при очень интенсивном трафике, но чуть увеличит потребление памяти. Для более равномерного распределения соединения по ячейкам хеш-таблицы и уменьшения коллизий, количество ячеек можно делать нечётным, а ещё лучше выбрать простое число (как советовал Дональд Кнут).

Так же внутри коннтрака живёт дополнительная таблица под названием __expectations__. Как можно догадаться из названия, в ней хранятся шаблоны потоков, которые порождаются основными потоками. О ней мы поговорим чуть позже, когда коснёмся трассировки протоколов прикладного уровня. Архитектурна она устроена аналогично основной таблице потоков. Так же на неё, соответственно, распространяются те же самые правила по выбору размеров, что и на основную таблицу.

Для того, чтобы просмотреть содержимое этих таблиц, можно использовать чтение из файлов /proc/, но лучше воспользоваться утилитой ```conntrack```, которая использует netlink-протокол для общения с ядром, а так же содержит встроенные средства фильтрации по различным параметрам, например, по протоколу и номерам портов. Подробности использования можно почитать в man'е (комманда ```conntrack -L```). В дополнение к таблицам __conntrack__ (основная таблица потоков) и __expect__ (таблица ожидаемых потоков), утилита позволяет так же посмотреть списки __dying__ (завершившиеся потоки, которые ещё не были удалены из таблицы) и __unconfirmed__ (новые потоки, которые ещё не добавлены в основную таблицу, так как оригинирующие их пакеты ещё не достигли хука, где поток будет подтверждён - см. далле). Особенность списков __dying__ и __unconfirmed__ в том, что они являются per-cpu, то есть у каждного ЦПУ в системе свои собственные экземпляры данных списков, и они не пересекаются друг с другом. В обычных условиях элементы находятся в этих двух списках очень непродолжительное время.


### Представление сетевых потоков в conntrack'е

Каждый сетевой поток в таблице коннтрака представлен в виде [структуры nf_conn](http://lxr.free-electrons.com/source/include/net/netfilter/nf_conntrack.h?v=4.7#L74), которая выглядит следующим образом (некоторые поля опущены):
```
struct nf_conn {
    ...
    struct nf_conntrack_tuple_hash tuplehash[IP_CT_DIR_MAX];
    unsigned long       status;
    struct timer_list   timeout;
    possible_net_t      ct_net;
    ...
    struct nf_conn     *master;
#if defined(CONFIG_NF_CONNTRACK_MARK)
    u_int32_t           mark;
#endif
    struct nf_ct_ext   *ext;
    union nf_conntrack_proto proto;
};
```

Разберёмся в назначении основных полей (хотя их назначение и так вполне понятно из названия):

*```struct nf_conntrack_tuple_hash tuplehash[IP_CT_DIR_MAX]``` - массив хеш-сумм кортежей потока. С помощью кортежа (англ. tuple) мы можем определить принадлежность пакета к определённому потоку. В общем случае в кортеже находятся адреса сетевого уровня, протокол следующего уровня и номера портов протокола траспортного уровня. Так как потоки у нас двунаправленные, то каждый поток представлен двумя записями в хеш-таблице - по одной на каждое направление потока. Направление представлено целочисленными константами __IP_CT_DIR_ORIG__ (направление потока от инициатора потока к пиру) и __IP_CT_DIR_REPLY__ (соответственно, обратное направление потока, к инициатору). Для каждого кортежа мы вычисляем хеш, чтобы знать местоположение структуры в хеш-таблице. Так же значения хешей сохраняем в самой структуре, чтобы уменьшить объём операций по поиску нужного потока в цепочке - сравниваем только сами хеши при поиске, и только в самом конце поиска проверяем кортеж целиком.

*```unsigned long status``` - состояние потока. В каждый момент времени поток может находиться в одном или нескольких состояниях, которые меняются в зависимости от передаваемого в данном потоке трафика.

*```struct timer_list timeout``` - таймер времени жизни потока и прикреплённая к нему функция обратного вызова с говорящим названием __dying_by_timeout__. 

*```possible_net_t ct_net``` - указатель на network namespace. Дело в том, что неймспейсы не имеют своих собственных таблиц потоков. Все потоки ото всех неймспейсов хранятся в одной таблице, а соответствие потоку неймспейску как раз определяется этим полем структуры. Тут возникает проблема исчерпания лимита потоков в хеш-таблице атакой на один из неймспейсов. В данный момент разработчики пытаются решить эту проблему, судя по информации с последнего netfilter workshop. 

*```struct nf_conn *master``` - для дополнительных потоков, которые были порождены из других потоков (дата-коннекты для фтп), тут указатель на это родительское соединение (контрольное соединение).

*```u_int32_t mark``` - метка потока. Может назначаться пользователем с помощью таргета CONNMARK, а потом реплицироваться в метку пакета (так, которая используется в сопоставлении ```-m mark``` и в правилах маршрутизации). О ней мы поговорим в самом конце, где будем разбирать различные практические трюки.

*```struct nf_ct_ext *ext``` - указатель на список расширений (extensions) коннтрака. Механизм расширений (как вы уже догадались) позволяет прицепить дополнительную информацию к потоку. Через них реализована трансляция адресов, аккаунтинг, механизм асинхронных оповещений и т.п.

*```union nf_conntrack_proto proto``` - в данном объединении хранится информация о потоке, которая используется обработчиками протокола внутри коннтрака. Например, для tcp тут хранится вся информация о текущем состоянии соединения, значения sequence-numbers, размер окна, таймеры и т.п. Подробно опишу всё это позже в главах, посвящённых обработчикам протоколов.

Собственно, это самое главное, что хранится в информации о каждом потоке. Кроме того, в данной структуре лежат ещё счётчик ссылок, номер цпу, на котором обрабатывается данный поток, метка контекста безопасности, если используется SELinux и кое-какая другая служебная информация.

Теперь рассмотрим то, какие состояния может иметь поток в коннтраке. Описывается оно значением поля __status__, является целочисленным значением, и принимает следующие значения (названия констант определены [тут]()):


Теперь поговорим о том, как создаются эти потоки в таблице коннтрака на основе анализа трафика, проходящего через сетевой стек.

### Прохождение пакетов и создание информации о потоке в коннтраке



