#Коннтрак

### Путь пакетов через ядро

Каждый кадр/ячейка/сетевой пакет (далее мы просто пакет, но надо учитывать контекст, чтобы не было путаницы) представлен в ядре двумя сущностями: собственно сами данные кадра/ячейки/пакета и структурой socket buffer, которая хранит в себе различную метаинформацию как о самом пакете (протокол, длина, указатели за различные заголовки, интерфейсы, через которые пакет пришёл или уйдёт и т.п.), так и внутриядерные служебные данные, ассоциированные с этим пакетом (счётчик ссылок, указатели на элементы списка для GSO, привязка к ЦПУ и очередям и т.п.). В ядре эта структура объявлена следующим образом - [тыц](http://lxr.free-electrons.com/source/include/linux/skbuff.h#L626) - там довольно-таки всё интуитивно понятно названо и, порой, даже прокоментировано. Более подробно человеческим языком о skbuff написано [тут](http://vger.kernel.org/~davem/skb.html). Это нам пригодится для дальнейшего погружения в потроха ядра.

Когда сетевая карта получает кадр по сети, она обычно размещает его в памяти (как правило используя DMA и кольцевой буфер), драйвер заполняет необходимые поля соответствующей пакету структуры skbuff и передаёт её дальше в сетевой стек, используя прерывание. Вот тут-то и начинается самое интересное, хе-хе.

Эта структура проходит через множество этапов обработки, передаётся в функции обработчиков протоколов различного уровня, у неё перезаписываются различные поля, перезаписываются сами данные пакета, и т.д., и т.п. Если нам нужен крутой файерволл, который может много всего, то он обязательно должен уметь обрабатывать трафик на различных этапах  обработки. А чтобы это делать, нужно этот трафик как-то перехватывать на всех этих этапах, и уже передавать в наш файерволл. В ядре линукс этот перехват трафика и реализован в подсистеме с названием netfilter. Эта подсистема предоставляет два внутриядерных интефейса (набора функций, структур и определений функций): один интерфейс для драйверов и обработчиков протоколов, благодаря чему разработчик может предусмотреть перенаправление трафика на обработку файерволлу, а второй интерфейс - для получения трафика на различных этапах обработки - для самой реализации файерволла. Сейчас в ядре существует две реализации файерволла - xtables, которая пришла на смену ipchains, и nftables, которая сейчас очень активно разрабатывается, и вообще суперпрогрессивная и производительная, в которой учтено большинство шишек, набитых во время реалиазации предыдущих реализаций (ух, как сложно и маркетинг-буллшитно получилось!).

Каждый этап обработки сетевого трафика представлен в netfilter в виде хука - точки перехвата. На следующей картинке я кратко нарисовал схему прохождения трафика через эти хуки.

![Путь пакета через сетевой стек](https://github.com/veryangryman/articles/raw/master/images/p_flow_brief2.png)

Кратко опишу все эти хуки, и их предназначение.

1. ingress - перехват на самом раннем этапе обработки трафика. Этот хук появился в ядре, начиная с версии 4.2 (т.е. относительно совсем недавно), и является альтернативой tc ingress queue, используемой для различных манипуляций с входящим трафиком (классификация, полисинг, перезапись, зеркалирование и перенаправление, и т.п.) Особенностью данного хука является то, что он привязывается к каждому сетевому интерфейсу независимо и опционально. Поддерживается только из nftables и позволяет делать ещё более интересные штуки с входящим трафиком (подробности и примеры использования в [списке рассылки](http://marc.info/?l=netfilter-devel&m=143033337020328&w=2)).

1. prerouting - на этом этапе системе ещё неизвестно назначение пакета, благодаря чему мы можем им управлять. После этого хука выполняется решение о маршрутизации (route decision). Для ipv4/ipv6 ищется соответствующий маршрут, а так же пакет пропускается через фильтр обратного пути (reverse path filter). Так как на этом этапе дальнейший маршрут ещё неизвестен, то, естественно, мы не знаем выходной интерфейс, а только входящий. Соотвественно, мы так же тут можем использовать только сопоставление по входящему интерфейсу (--input-interface).

В результате route decision мы можем найти следующие типы маршрутов, о которых зависит дальнейшее прохождение пакета:
*. blackhole/unreachable/prohibit - пакет отбрасывается молча или с передачей отправителю пакета icmp
*. local/broadcast - пакет предназначен самому хосту, и передаётся обработчику протокола следующего уровня. Например, для транспортных протоколов будет искаться соответствующий сокет.
*. unicast/multicast - пакет предназначен другому хосту, т.е. является транзитным. Этот пакет будет отправлен в сеть.

1. input - в этот хук попадает трафик, предназначенный самом хосту. После этого трафик идёт к локальному процессу. После этого хука идёт поиск соответствующего сокета и политик безопасности для ipsec и т.п. Причём, для ipsec трафика идёт расшифровка и декапсуляция, и декапсулированные пакеты направляются снова на этап prerouting. Таким образом входящий ipsec-трафик проходит через хук prerouting дважды.

1. forward - через этот хук проходит транзитный трафик. Так как у нас уже есть решение о маршрутизации, то мы знаем выходной интерфейс. Благодаря этому мы можем использовать сопоставления --input-interface и --output-interface.

1. output - только трафик, оригинированный данным хостом, проходит через этот хук. Для новых соединений мы сразу знаем выходной интерфейс и адрес источника, так как решение о маршрутизации принимается в момент передачи данных через сокет от процесса в сетевой стек, при инкапсулации данных на сетевом уровне. Так же тут перепроверка решения о маршрутизации (reroute check).

1. postrouting - последний хук, через который идёт сетевой трафик. На этом этапе так же известен выходной интерфейс, но для, например, оригинированного хостом трафика, входного интерфейса в принципе нет, поэтому в данном хуке мы можем использовать сопоставление по выходному интерфейсу (--output-interface). После хука postrouting трафик так же проходит через политики безопасности. Трафик, попавший под политики, инкапсулируется и шифруется, а потом становится локально-оригинированным, и, соответственно, попадает в хук output.

Следом после всех этих этапов, трафик проходит через подсистему контроля трафика (traffic control - tc), где осуществляется классификация, если необходимо, а так же шейпирование/полисинг (shaping/policing, так же используется более общий термпин scheduling - планирование). После этого трафик передаётся снова в драйвер сетевой карты для последующей передачи в сеть.

Мы кратко рассмотрели хуки, и порядок прохождения сетевого трафика через них. Но что они представляют собой? На самом деле внутри netfilter каждый хук представлен упорядоченным списком. Каждый элемент списка содержит указатель на функцию обратного вызова (callback), которой передаётся структура skbuff пакета и ещё некоторые дополнительные аргументы. 



